{
 "cells": [
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T04:02:17.809848600Z",
     "start_time": "2026-01-12T04:01:45.279313200Z"
    }
   },
   "source": [
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Unset SPARK_HOME to ensure the pyspark library uses its own bundled Spark runtime,\n",
    "# preventing conflicts with any globally installed Spark versions.\n",
    "# Please restart the Jupyter kernel for this change to take effect.\n",
    "os.environ.pop('SPARK_HOME', None)\n",
    "\n",
    "## DEFINE VARIABLES\n",
    "CATALOG_URI = \"http://nessie:19120/api/v2\"\n",
    "WAREHOUSE = \"s3://lakehouse/\"\n",
    "\n",
    "POSTGRES_DB = os.environ.get(\"POSTGRES_DB\")\n",
    "POSTGRES_JDBC_URL = f\"jdbc:postgresql://postgres:5432/{POSTGRES_DB}\"\n",
    "POSTGRES_USER = os.environ.get(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.environ.get(\"POSTGRES_PASSWORD\")\n",
    "\n",
    "MYSQL_DB = os.environ.get(\"MYSQL_DB\")\n",
    "MYSQL_JDBC_URL = f\"jdbc:mysql://mysql:3306/{MYSQL_DB}\"\n",
    "MYSQL_USER = os.environ.get(\"MYSQL_USER\")\n",
    "MYSQL_PASSWORD = os.environ.get(\"MYSQL_PASSWORD\")\n",
    "\n",
    "STORAGE_URI = \"http://minio:9000\"\n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "## CONFIGURE SPARK SESSION\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName('Iceberg Ingestion')\n",
    "    .set('spark.jars.packages',\n",
    "         'org.postgresql:postgresql:42.7.3,'\n",
    "         'mysql:mysql-connector-java:8.0.33,'\n",
    "         'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "         'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "         'software.amazon.awssdk:bundle:2.24.8,'\n",
    "         'software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "    .set('spark.sql.extensions',\n",
    "         'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "         'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "    .set('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "    .set('spark.sql.catalog.iceberg.uri', CATALOG_URI)\n",
    "    .set('spark.sql.catalog.iceberg.ref', 'main')\n",
    "    .set('spark.sql.catalog.iceberg.authentication.type', 'NONE')\n",
    "    .set('spark.sql.catalog.iceberg.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "    .set('spark.sql.catalog.iceberg.warehouse', WAREHOUSE)\n",
    "    .set('spark.sql.catalog.iceberg.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "    .set('spark.sql.catalog.iceberg.client.region', AWS_REGION)\n",
    "    .set('spark.sql.catalog.iceberg.s3.endpoint', STORAGE_URI)\n",
    "    .set('spark.sql.catalog.iceberg.s3.access-key-id', AWS_ACCESS_KEY)\n",
    "    .set('spark.sql.catalog.iceberg.s3.secret-access-key', AWS_SECRET_KEY)\n",
    "    .set('spark.sql.catalog.iceberg.s3.path-style-access', 'true')\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", STORAGE_URI)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ")\n",
    "\n",
    "## START SPARK SESSION\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running @ \" + datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running @ 2026-01-11 23:02:17\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "adafbf47a9fb2e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T04:02:56.942394400Z",
     "start_time": "2026-01-12T04:02:56.704179300Z"
    }
   },
   "source": [
    " # Define the JDBC connection properties\n",
    "# properties = {\n",
    "#     \"user\": POSTGRES_USER,\n",
    "#     \"password\": POSTGRES_PASSWORD,\n",
    "#     \"driver\": \"org.postgresql.Driver\"\n",
    "# }\n",
    "\n",
    "properties = {\n",
    "    \"user\": MYSQL_USER,\n",
    "    \"password\": MYSQL_PASSWORD,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Read the sales_data table from RDBMS into a Spark DataFrame\n",
    "transactions_df = spark.read.jdbc(url=MYSQL_JDBC_URL, table=\"poc.transactions\", properties=properties)\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "transactions_df.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "|     id|transaction_date|transaction_type|posted_date|     description|amount|\n",
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "|txn_001|      2024-01-01|          credit| 2024-01-01| Initial Deposit| 500.0|\n",
      "|txn_002|      2024-01-02|           debit| 2024-01-03|     Coffee Shop|   4.0|\n",
      "|txn_003|      2024-01-05|           debit| 2024-01-06|Online Bookstore| 20.99|\n",
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "bbbee8c67336ea64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T04:03:07.364611Z",
     "start_time": "2026-01-12T04:03:04.446485300Z"
    }
   },
   "source": [
    "# Manipulate the data\n",
    "# Multiply each amount by 2\n",
    "transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\") * 2)\n",
    "\n",
    "# Create a namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.poc;\")\n",
    "\n",
    "# Write the DataFrame to an Iceberg table in the Nessie catalog\n",
    "transactions_df.writeTo(\"iceberg.poc.transactions\").createOrReplace()\n",
    "\n",
    "# Verify that the data was written to Iceberg by reading the table\n",
    "spark.read.table(\"iceberg.poc.transactions\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "|     id|transaction_date|transaction_type|posted_date|     description|amount|\n",
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "|txn_001|      2024-01-01|          credit| 2024-01-01| Initial Deposit|1000.0|\n",
      "|txn_002|      2024-01-02|           debit| 2024-01-03|     Coffee Shop|   8.0|\n",
      "|txn_003|      2024-01-05|           debit| 2024-01-06|Online Bookstore| 41.98|\n",
      "+-------+----------------+----------------+-----------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed9132405854b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
